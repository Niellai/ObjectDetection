{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.layers import Conv2D, Reshape, Flatten, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, GlobalAveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.backend import epsilon\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ###\n",
    "Building up our training set, validation set and images from Pascal 2007 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'type', 'categories', 'annotations'])\n",
      "\n",
      "images: {'width': 500, 'id': 12, 'file_name': '000012.jpg', 'height': 333}\n",
      "\n",
      "annotations: {'area': 34104, 'iscrowd': 0, 'bbox': [155, 96, 196, 174], 'category_id': 7, 'id': 1, 'segmentation': [[155, 96, 155, 270, 351, 270, 351, 96]], 'ignore': 0, 'image_id': 12}\n",
      "\n",
      "categories: {'id': 1, 'name': 'aeroplane', 'supercategory': 'none'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset/Annotations\")\n",
    "image_path = os.path.join(os.getcwd(), \"dataset/JPEGImages\")\n",
    "\n",
    "train_path = os.path.join(dataset_path, \"pascal_train2007.json\")\n",
    "val_path = os.path.join(dataset_path, \"pascal_val2007.json\")\n",
    "\n",
    "# building up training dataset\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_dataset = []\n",
    "for image in train_data[\"images\"]:\n",
    "    image_set = {}\n",
    "    image_set[\"image\"] = image\n",
    "    \n",
    "    anno_list = []\n",
    "    for anno in train_data[\"annotations\"]:\n",
    "        if anno[\"image_id\"] == image['id']:\n",
    "            anno_list.append(anno)\n",
    "    image_set[\"annotation\"] = sorted(anno_list, key=lambda x:x[\"area\"], reverse=True)[0]\n",
    "    train_dataset.append(image_set)\n",
    "\n",
    "\n",
    "# building up validation dataset    \n",
    "with open(val_path) as f:\n",
    "    val_data = json.load(f)\n",
    "    \n",
    "val_dataset = []\n",
    "for image in val_data[\"images\"]:\n",
    "    image_set = {}\n",
    "    image_set[\"image\"] = image\n",
    "    \n",
    "    anno_list = []\n",
    "    for anno in val_data[\"annotations\"]:\n",
    "        if anno[\"image_id\"] == image[\"id\"]:\n",
    "            anno_list.append(anno)\n",
    "    image_set[\"annotation\"] = sorted(anno_list, key=lambda x:x[\"area\"], reverse=True)[0]\n",
    "    val_dataset.append(image_set)\n",
    "\n",
    "# redistribute list\n",
    "combine_list = train_dataset + val_dataset\n",
    "train_dataset = combine_list[:int(len(combine_list) * 0.8)]\n",
    "val_dataset = combine_list[int(len(combine_list) * 0.8):]\n",
    "    \n",
    "# converter to convert id to label name\n",
    "id_to_name = {c['id']:c['name'] for c in train_data['categories']}\n",
    "\n",
    "print(train_data.keys())\n",
    "print()\n",
    "print(\"images: {}\\n\".format(train_data['images'][0]))\n",
    "print(\"annotations: {}\\n\".format(train_data['annotations'][0]))\n",
    "print(\"categories: {}\\n\".format(train_data['categories'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine dataset: 5011\n",
      "Train dataset: 4008\n",
      "Validate dataset: 1003\n"
     ]
    }
   ],
   "source": [
    "print(\"Combine dataset: {}\".format(len(combine_list)))\n",
    "print(\"Train dataset: {}\".format(len(train_dataset)))\n",
    "print(\"Validate dataset: {}\".format(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_hw(a): \n",
    "    x0 = bbox[0]\n",
    "    y0 = height - bbox[1] - bbox[3]\n",
    "    x1 = bbox[2]\n",
    "    y1 = bbox[3]\n",
    "    return  [x0, y0, x1, y1]\n",
    "\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "    \n",
    "def draw_rect(ax, b):\n",
    "    # b = bb_hw(b)\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "    \n",
    "def draw_text(ax, xy, txt, sz=14):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "    \n",
    "def show_img(im, figsize=(10, 10), ax=None):    \n",
    "    if not ax: fig, ax = plt.subplots(figsize=figsize)            \n",
    "    ax.imshow(im)    \n",
    "    plt.gca().invert_yaxis()    \n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "    return ax\n",
    "\n",
    "def load_image(path):            \n",
    "    img = cv2.imread(img_path) # read as color image\n",
    "    img = img[::-1,:,:] # revise height in (height, width, channel)\n",
    "    img = img[...,::-1] # flip color\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interate through train_dataset by pressing any keys and press 'q' to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155, 96, 196, 174]\n",
      "[89, 77, 314, 259]\n",
      "[2, 1, 241, 461]\n",
      "[89, 124, 248, 88]\n",
      "[103, 77, 272, 106]\n",
      "[8, 106, 491, 157]\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(train_dataset):\n",
    "    data = train_dataset[idx]\n",
    "    img_path = os.path.join(image_path, data[\"image\"]['file_name'])\n",
    "    height = data[\"image\"][\"height\"]\n",
    "    width = data[\"image\"][\"width\"]\n",
    "\n",
    "    bbox = data['annotation']['bbox']\n",
    "    # bbox = bb_hw(bbox)\n",
    "    print(bbox)\n",
    "\n",
    "    img = cv2.imread(img_path, 1)\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0,255,0), 2)\n",
    "    cv2.imshow(\"image\", img)\n",
    "\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def aug_image(img, bbox):\n",
    "    # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "    # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "    seq = iaa.Sequential(\n",
    "        [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.2), # vertically flip 20% of all images                    \n",
    "            \n",
    "            # execute 0 to 3 of the following (less important) augmenters per image\n",
    "            # don't execute all of them, as that would often be way too strong\n",
    "            iaa.SomeOf((0, 3),\n",
    "                [\n",
    "                    sometimes(iaa.Affine(rotate=(-45, 45))),\n",
    "                    # sometimes(iaa.Superpixels(p_replace=(0, 0.3), n_segments=(0, 100))), # convert images into their superpixel representation\n",
    "                    iaa.OneOf([\n",
    "                        iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                        iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                        iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                    ]),\n",
    "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                    # iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "                    # search either for all edges or for directed edges,\n",
    "                    # blend the result with the original image using a blobby mask\n",
    "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                    ])),\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                        iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                    ]),\n",
    "                    iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "                    iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                    iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "                    # either change the brightness of the whole image (sometimes\n",
    "                    # per channel) or change the brightness of subareas\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
    "                        iaa.FrequencyNoiseAlpha(\n",
    "                            exponent=(-4, 0),\n",
    "                            first=iaa.Multiply((0.5, 1.5), per_channel=True),\n",
    "                            second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "                        )\n",
    "                    ]),\n",
    "                    iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                    iaa.Grayscale(alpha=(0.0, 1.0)),\n",
    "                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.04))), # sometimes move parts of the image around\n",
    "                    # sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.05)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ],\n",
    "        random_order=True\n",
    "    )\n",
    "    \n",
    "    # take note that pascal bbox is in x1, y1, width ,height\n",
    "    # bbox should hold x1, y1, x2, y2\n",
    "    bbs = ia.BoundingBoxesOnImage([\n",
    "        ia.BoundingBox(x1=bbox[0], y1=bbox[1], x2=bbox[0]+bbox[2], y2=bbox[1]+bbox[3]),    \n",
    "    ], shape=img.shape)\n",
    "    \n",
    "    # create fix augment sequence\n",
    "    seq_det = seq.to_deterministic()\n",
    "\n",
    "    image_aug = np.array(seq_det.augment_images([img])[0])\n",
    "    bbs_aug = seq_det.augment_bounding_boxes([bbs])[0]\n",
    "    \n",
    "    bbox_list = []\n",
    "    for i in range(len(bbs.bounding_boxes)):\n",
    "        bbox_after = bbs_aug.bounding_boxes[i]\n",
    "        bbox_after = [bbox_after.x1, bbox_after.y1, bbox_after.x2, bbox_after.y2]\n",
    "        bbox_list.append(bbox_after)\n",
    "    # take note that return augmented bbox result is in x1, y1, x2, y2\n",
    "    return image_aug, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 300\n",
    "\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    img_path = os.path.join(image_path, data[\"image\"]['file_name'])\n",
    "\n",
    "    bbox = data['annotation']['bbox']\n",
    "    img = cv2.imread(img_path, 1)\n",
    "    width = data[\"image\"][\"width\"]\n",
    "    height = data[\"image\"][\"height\"]\n",
    "\n",
    "    # rescale image\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)) \n",
    "    x_scale = IMAGE_SIZE / width\n",
    "    y_scale = IMAGE_SIZE / height\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    bbox = [int(x0 * x_scale), int(y0 * y_scale), int(x1 * x_scale), int(y1 * y_scale)]\n",
    "    \n",
    "    # augment image and bbox\n",
    "    img, bbox_list = aug_image(img, bbox)\n",
    "    bbox = bbox_list[0]\n",
    "\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "    cv2.imshow(\"image\", img)\n",
    "\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
